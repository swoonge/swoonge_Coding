{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "# import torch.optim as optim\n",
    "# import torch.nn.init as init\n",
    "\n",
    "# 추가적인 모듈\n",
    "import torchvision\n",
    "from torch.utils import data # Data의 batch size 설정 및 random하게 섞기 등을 해주는 모듈\n",
    "import torchsummary\n",
    "\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32*8\n",
    "learning_rate = 0.001\n",
    "num_epoch = 30\n",
    "\n",
    "transform = torchvision.transforms.Compose(\n",
    "    torchvision.transforms.ToTensor(),  # convert image to Tensor\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "train_data=torchvision.datasets.CIFAR10(\"../DataSets/\", train=True, transform=torchvision.transforms.ToTensor(), target_transform=None, download=True)\n",
    "test_data=torchvision.datasets.CIFAR10(\"../DataSets/\", train=False, transform=torchvision.transforms.ToTensor(), target_transform=None, download=True)\n",
    "\n",
    "dataset_size = len(train_data)\n",
    "train_size = int(dataset_size * 0.8)\n",
    "test_size = int(batch_size)\n",
    "validation_size = dataset_size - train_size - test_size\n",
    "\n",
    "train_dataset, val_dataset, train_test = data.random_split(train_data, [train_size, validation_size, test_size])\n",
    "\n",
    "train_loader = data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=6, drop_last=True)\n",
    "val_loader = data.DataLoader(val_dataset, batch_size=batch_size, shuffle=True, num_workers=6, drop_last=True)\n",
    "test_loader = data.DataLoader(test_data, batch_size=batch_size, shuffle=True, num_workers=6, drop_last=True)\n",
    "train_test_loader = data.DataLoader(train_test, batch_size=batch_size, shuffle=True, num_workers=2, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset CIFAR10\n",
      "    Number of datapoints: 50000\n",
      "    Root location: ../DataSets/\n",
      "    Split: Train\n",
      "    StandardTransform\n",
      "Transform: ToTensor()\n",
      "<torch.utils.data.dataset.Subset object at 0x7f6c3b591820>\n",
      "Dataset CIFAR10\n",
      "    Number of datapoints: 10000\n",
      "    Root location: ../DataSets/\n",
      "    Split: Test\n",
      "    StandardTransform\n",
      "Transform: ToTensor()\n",
      "torch.Size([3, 32, 32])\n",
      "<torch.utils.data.dataloader.DataLoader object at 0x7f6c3b591730>\n"
     ]
    }
   ],
   "source": [
    "print(train_data)\n",
    "print(val_dataset)\n",
    "print(test_data)\n",
    "print(train_data[0][0].shape)\n",
    "print(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BottleNeck(nn.Module):\n",
    "    expansion = 4\n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.residual_function = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(out_channels, out_channels * BottleNeck.expansion, kernel_size=1, stride=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels * BottleNeck.expansion),\n",
    "        )\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        # 점선 화살표 부분 맞춰는 코드\n",
    "        # stride가 1이 아니거나(outplane이 /2로 작아지거나) or in_channel이 out_channel*expansion()\n",
    "        if stride != 1 or in_channels != out_channels * BottleNeck.expansion:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels*BottleNeck.expansion, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(out_channels*BottleNeck.expansion)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.residual_function(x) + self.shortcut(x)\n",
    "        x = self.relu(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResCRNN_CIFAR(nn.Module):\n",
    "    def __init__(self, num_block: int = [3, 3], num_classes: int = 10, init_weights: bool = True) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.in_channels = 64\n",
    "        self.inplanes = 64\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(3, self.inplanes, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(self.inplanes),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
    "        )\n",
    "\n",
    "        self.conv2_x = self._make_layer(BottleNeck, 64, num_block[0], 1)\n",
    "        self.conv3_x = self._make_layer(BottleNeck, 128, num_block[1], 2)\n",
    "        self.c01 = nn.Sequential(\n",
    "            nn.Conv2d(128 * BottleNeck.expansion, 128, kernel_size=1, stride=1, bias=False),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 64, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 16, kernel_size=1, stride=1, bias=False),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(16, 1, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(1),\n",
    "            nn.ReLU()\n",
    "        ) #[batch,1,8,8]\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size=8, hidden_size=20, num_layers=1, dropout=0.3, batch_first=True)\n",
    "\n",
    "        self.fc = nn.Linear(160, num_classes)\n",
    "\n",
    "        # weights inittialization\n",
    "        if init_weights:\n",
    "            self._initialize_weights()\n",
    "\n",
    "    def _make_layer(self, block, out_channels, num_blocks, stride):\n",
    "        strides = [stride] + [1] * (num_blocks - 1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.inplanes, out_channels, stride))\n",
    "            self.inplanes = out_channels * block.expansion\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x, \n",
    "                h0=torch.zeros(1, 2, 20, requires_grad=True).to(torch.device('cuda')), \n",
    "                c0=torch.zeros(1, 2, 20, requires_grad=True).to(torch.device('cuda'))):\n",
    "        output = self.conv1(x)\n",
    "        output = self.conv2_x(output)\n",
    "        output = self.conv3_x(output)\n",
    "        output = self.c01(output) \n",
    "        # print(output.shape) # shape = [batch,1,8,8]\n",
    "        output = output.view([output.shape[0], 8, 8])\n",
    "        # print(output.shape) # shape = [batch,8(seq),8(data)]\n",
    "        # print(hidden.shape) # shape = [1, batch, 20]\n",
    "        outputs, (hn, cn) = self.lstm(output, (h0, c0))\n",
    "        # output = torch.cat((output11, output21, output31, output41), dim=1)       \n",
    "        # print(hidden.shape)\n",
    "        # print(hiddens.shape)\n",
    "        # output = self.final_conv(output)\n",
    "\n",
    "        outputs = torch.flatten(outputs, 1)\n",
    "        # print(outputs.shape)\n",
    "        outputs = self.fc(outputs)\n",
    "        return outputs\n",
    "\n",
    "    # define weight initialization function\n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.normal_(m.weight, 0, 0.01)\n",
    "                nn.init.constant_(m.bias, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vision/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:67: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([256, 10])\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = ResCRNN_CIFAR([3, 5]).to(device)\n",
    "\n",
    "# 모델이 잘 통과하는지 확인\n",
    "x = torch.randn(batch_size, 3, 32, 32).to(device)\n",
    "h0 = torch.zeros(1, batch_size, 20, requires_grad=True).to(device)\n",
    "c0 = torch.zeros(1, batch_size, 20, requires_grad=True).to(device)\n",
    "output = model(x, h0, c0)\n",
    "print(output.size())\n",
    "\n",
    "# 모델 summary\n",
    "# torchsummary.summary(model, (3, 32, 32), device=device.type)\n",
    "# torchsummary.summary(model, (1, 28, 28), hidden, device=device.type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = ResCRNN_CIFAR([3,5]).to(device)\n",
    "\n",
    "loss_func=nn.CrossEntropyLoss()\n",
    "optimizer=torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "# scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer=optimizer,\n",
    "#                                                 epochs=num_epoch,\n",
    "#                                                 max_lr=0.01,\n",
    "#                                                 pct_start=0.3,\n",
    "#                                                 steps_per_epoch=1\n",
    "#                                                 )\n",
    "loss_array = []\n",
    "accuracy_array = []\n",
    "\n",
    "test_model_flag = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 is start\n",
      "Accuracy of Test Data: 10.968, loss: 2.3010,  lr: 0.001000\n",
      "Accuracy of Test Data: 11.338, loss: 2.3000,  lr: 0.001000\n",
      "epoch 1 is start\n",
      "Accuracy of Test Data: 10.998, loss: 2.3011,  lr: 0.001000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 15\u001b[0m\n\u001b[1;32m     13\u001b[0m output \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mforward(x, h0, c0)\n\u001b[1;32m     14\u001b[0m loss \u001b[39m=\u001b[39m loss_func(output, y_)\n\u001b[0;32m---> 15\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m     16\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n\u001b[1;32m     18\u001b[0m \u001b[39mif\u001b[39;00m ((j \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m) \u001b[39mif\u001b[39;00m test_model_flag \u001b[39mis\u001b[39;00m \u001b[39mTrue\u001b[39;00m \u001b[39melse\u001b[39;00m (j \u001b[39m%\u001b[39m \u001b[39m100\u001b[39m \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m)):\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/_tensor.py:488\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    478\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    479\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    480\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    481\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    486\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    487\u001b[0m     )\n\u001b[0;32m--> 488\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    489\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    490\u001b[0m )\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/autograd/__init__.py:197\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    192\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    194\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    195\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    196\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 197\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    198\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    199\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_datas = train_test_loader if test_model_flag is True else train_loader\n",
    "lrs = []\n",
    "val_rnn_acc = 0\n",
    "for i in range(num_epoch):\n",
    "    print(\"epoch\", i, \"is start\")\n",
    "    for j, [img, label] in enumerate(train_datas):\n",
    "        x = img.to(device)\n",
    "        y_ = label.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        h0 = torch.zeros(1, batch_size, 20, requires_grad=True).to(device)\n",
    "        c0 = torch.zeros(1, batch_size, 20, requires_grad=True).to(device)\n",
    "        output = model.forward(x, h0, c0)\n",
    "        loss = loss_func(output, y_)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if ((j == 0) if test_model_flag is True else (j % 100 == 0)):\n",
    "            loss_array.append(loss.detach().cpu().numpy())\n",
    "            val_rnn_acc = utils.cal_lstm_acc(model=model,\n",
    "                                    datas=train_test_loader if test_model_flag is True else test_loader,\n",
    "                                    device=device,\n",
    "                                    h0=torch.zeros(1, batch_size, 20, requires_grad=True).to(device),\n",
    "                                    c0=torch.zeros(1, batch_size, 20, requires_grad=True).to(device))\n",
    "            print('Accuracy of Test Data: {0:.3f}, loss: {1:.4f},  lr: {2:.6f}'.format(val_rnn_acc, loss_array[-1], optimizer.param_groups[0]['lr']))\n",
    "            accuracy_array.append(val_rnn_acc.detach().cpu().numpy())\n",
    "            if val_rnn_acc > 99.5:\n",
    "                break\n",
    "    lrs.append(optimizer.param_groups[0][\"lr\"])\n",
    "    # scheduler.step()\n",
    "    if val_rnn_acc > 99.5:\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils\n",
    "utils.plot_acc_loss(accuracy_array=accuracy_array, loss_array=loss_array, lrs=lrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testdata_acc = utils.cal_rnn_acc(model=model,\n",
    "                                    datas= test_loader,\n",
    "                                    device=device,\n",
    "                                    hidden = torch.zeros(1, batch_size, 20, requires_grad=True).to(device))\n",
    "print(\"Accuracy of Test Data: {}\".format(testdata_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
