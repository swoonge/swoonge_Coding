{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/zhulf0804/GCN.PyTorch/tree/master\n",
    "\n",
    "https://relational.fit.cvut.cz/dataset/CORA\n",
    "\n",
    "https://m.blog.naver.com/winddori2002/222183504185\n",
    "\n",
    "https://chioni.github.io/posts/gnn/"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.datasets.Planetoid.html#torch_geometric.datasets.Planetoid"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://baeseongsu.github.io/posts/pytorch-geometric-introduction/"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " data_load 함수는 citation network인 cora 데이터셋을 load하는 함수입니다. 데이터의 구성은 노드(논문) features(논문에서 사용하는 단어사전 1433개), label(문서 분류), edge(인용 관계)입니다. 여기서 features 1433개는 one-hot vector로 표현됩니다.\n",
    "\n",
    "위의 함수에서 핵심은 각 노드와 엣지 데이터를 기반으로 symmertric adjacency matrix를 만들고 indentity matrix와 더해줍니다. 또한, train, val, test에 index를 지정하여 semi-supervised transductive를 사용할 수 있도록 합니다. 이 때의 index들은 이후 loss를 계산할 때 train index만 적용하기 위함입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<<features>>\n",
      "features is same?: False, but dtype and shape is same. (torch.float64, torch.float64)\n",
      "--------------------------------------------------\n",
      "<<y>>\n",
      "y_train, test, val is same?: False\n",
      "True node is 140, flase node is 2568. because train set is 140개\n",
      "False node is y_train: tensor([0., 0., 0., 0., 0., 0., 0.], dtype=torch.float64), loader_y_train: tensor([0., 0., 0., 0., 1., 0., 0.], dtype=torch.float64)\n",
      "즉, 각 y_**들은 각자 val, train, test 데이터셋에 대한 부분만 라벨이 블라인드 되어 있는 label들이다. 따라서 전체 길이는 모두 같다.\n",
      "--------------------------------------------------\n",
      "<<mask>>\n",
      "train, test, val_mask is same?: True\n"
     ]
    }
   ],
   "source": [
    "# DataSet\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch_geometric.datasets import Planetoid # The citation network datasets \"Cora\", \"CiteSeer\" and \"PubMed\" from the \"Revisiting Semi-Supervised Learning with Graph Embeddings\" paper.\n",
    "from torch_geometric.utils import to_dense_adj\n",
    "from datasets import load_data\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "# 데이터들을 로드. loader_dataset은 torch_geometric.datasets 모듈로 로드\n",
    "loader_dataset = 'cora'\n",
    "loader_dataset = Planetoid(root='../../datasets/Cora', name='Cora')\n",
    "adj, features, y_train, y_val, y_test, train_mask, val_mask, test_mask = load_data(\"../../datasets/Cora/Cora/raw\", \"cora\")\n",
    "\n",
    "# loader_dataset에서 load_data에서 로드한 데이터와 같은 형식으로 편집\n",
    "loader_adj = to_dense_adj(loader_dataset[0].edge_index).squeeze(0).double()\n",
    "loader_features = loader_dataset[0].x.double()\n",
    "# y는 one hot encoding되어 있음\n",
    "loader_y_train = F.one_hot(loader_dataset[0].y).double()\n",
    "loader_y_val = loader_dataset[0].y[loader_dataset[0].val_mask].double()\n",
    "loader_y_test = loader_dataset[0].y[loader_dataset[0].test_mask].double()\n",
    "loader_train_mask = loader_dataset[0].train_mask\n",
    "loader_val_mask = loader_dataset[0].val_mask\n",
    "loader_test_mask = loader_dataset[0].test_mask\n",
    "#########################################\n",
    "\n",
    "# print(\"adj is same?: {}\".format(np.allclose(adj, loader_adj)))\n",
    "\n",
    "print(\"<<features>>\")\n",
    "print(\"features is same?: {}, but dtype and shape is same. ({}, {})\".format(np.allclose(features, loader_features), features.dtype, loader_features.dtype))\n",
    "print(\"-\"*50)\n",
    "print(\"<<y>>\")\n",
    "print(\"y_train, test, val is same?: {}\".format(torch.eq(y_train, loader_y_train).all()))\n",
    "t = [0, 0]\n",
    "f = [0, 0]\n",
    "for i in range(len(y_train)):\n",
    "    if torch.eq(y_train[i], loader_y_train[i]).all(): \n",
    "        t[0] += 1\n",
    "        if t[0] == 1: t[1] = i\n",
    "    else:\n",
    "        f[0] += 1\n",
    "        if f[0] == 1: f[1] = i\n",
    "print(\"True node is {}, flase node is {}. because train set is 140개\".format(t[0], f[0]))\n",
    "print(\"False node is y_train: {}, loader_y_train: {}\".format(y_train[f[1]], loader_y_train[f[1]]))\n",
    "print(\"즉, 각 y_**들은 각자 val, train, test 데이터셋에 대한 부분만 라벨이 블라인드 되어 있는 label들이다. 따라서 전체 길이는 모두 같다.\")\n",
    "print(\"-\"*50)\n",
    "print(\"<<mask>>\")\n",
    "print(\"train, test, val_mask is same?: {}\".format(torch.eq(train_mask, loader_train_mask).all()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<<Cora Dataset Info>>\n",
      " --------------------------------------------------\n",
      "Cora Dataset은 여러 그래프가 모인 데이터셋이 아닌, 통채로 큰 그래프인 데이터셋이므로(for node level gnn) Cora[0] 그래프 하나만 가지고 있다.\n",
      "- Cora: Cora(), len = 1\n",
      "- num_classes: 7\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "- dataset[0].keys: ['edge_index', 'x', 'test_mask', 'y', 'val_mask', 'train_mask']\n",
      "--------------------------------------------------\n",
      "- node: torch.Size([2708, 1433])\n",
      "- num_node: 2708, 2708\n",
      "- num_node_features: 1433, 1433\n",
      "--------------------------------------------------\n",
      "- edges: torch.Size([2, 10556])\n",
      "- num_edges: 10556, 10556\n",
      "--------------------------------------------------\n",
      "- class: tensor([3, 4, 4,  ..., 3, 3, 3])\n",
      "- class_set: tensor([0, 1, 2, 3, 4, 5, 6])\n",
      "--------------------------------------------------\n",
      "- num_train_mask: 140\n",
      "- num_val_mask: 500\n",
      "- num_test_mask: 1000\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(\"<<Cora Dataset Info>>\\n\", \"-\"*50)\n",
    "print(\"Cora Dataset은 여러 그래프가 모인 데이터셋이 아닌, 통채로 큰 그래프인 데이터셋이므로(for node level gnn) Cora[0] 그래프 하나만 가지고 있다.\")\n",
    "print(\"- Cora: {}, len = {}\".format(loader_dataset, len(loader_dataset)))\n",
    "print(\"- num_classes: {}\".format(loader_dataset.num_classes))\n",
    "print(\"-\"*50)\n",
    "print(\"-\"*50)\n",
    "print(\"- dataset[0].keys: {}\".format(loader_dataset[0].keys))\n",
    "print(\"-\"*50)\n",
    "print(\"- node: {}\".format(loader_dataset[0].x.shape))\n",
    "print(\"- num_node: {}, {}\".format(loader_dataset[0].num_nodes, len(loader_dataset[0].x)))\n",
    "print(\"- num_node_features: {}, {}\".format(loader_dataset[0].num_node_features, len(loader_dataset[0].x[0])))\n",
    "print(\"-\"*50)\n",
    "print(\"- edges: {}\".format(loader_dataset[0].edge_index.shape))\n",
    "print(\"- num_edges: {}, {}\".format(loader_dataset[0].num_edges, len(loader_dataset[0].edge_index[1])))\n",
    "print(\"-\"*50)\n",
    "print(\"- class: {}\".format(loader_dataset[0].y))\n",
    "print(\"- class_set: {}\".format(torch.unique(loader_dataset[0].y)))\n",
    "print(\"-\"*50)\n",
    "print(\"- num_train_mask: {}\".format(loader_dataset[0].train_mask.sum().item()))\n",
    "print(\"- num_val_mask: {}\".format(loader_dataset[0].val_mask.sum().item()))\n",
    "print(\"- num_test_mask: {}\".format(loader_dataset[0].test_mask.sum().item()))\n",
    "print(\"-\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.gcn import GCN\n",
    "from models.utils import build_optimizer, get_loss, get_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_dim = 16\n",
    "dropout = 0.5\n",
    "init_lr = 0.01\n",
    "weight_decay = 5e-4\n",
    "epoches = 200\n",
    "log_interval = 10\n",
    "checkpoint_interval = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input of GCN model: input_dim = 1433, hidden_dim = 16, num_classes = 7, dropout = 0.5\n"
     ]
    }
   ],
   "source": [
    "print(\"Input of GCN model: input_dim = {}, hidden_dim = {}, num_classes = {}, dropout = {}\".format(loader_dataset.num_node_features, hidden_dim, loader_dataset.num_classes, dropout))\n",
    "\n",
    "# device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = GCN(loader_dataset.num_node_features, hidden_dim, loader_dataset.num_classes, dropout)\n",
    "optimizer = build_optimizer(model, init_lr, weight_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    for epoch in range(epoches + 1):\n",
    "        outputs = model(adj, features)\n",
    "        loss = get_loss(outputs, y_train, train_mask)\n",
    "        val_loss = get_loss(outputs, y_val, val_mask).detach().numpy()\n",
    "        model.eval()\n",
    "        outputs = model(adj, features)\n",
    "        train_accuracy = get_accuracy(outputs, y_train, train_mask)\n",
    "        val_accuracy = get_accuracy(outputs, y_val, val_mask)\n",
    "        model.train()\n",
    "        # print('loss', {'train_loss': loss.detach().numpy(), 'val_loss': val_loss}, epoch)\n",
    "        # print('accuracy', {'train_ac': train_accuracy, 'val_ac': val_accuracy}, epoch)\n",
    "        if epoch % log_interval == 0:\n",
    "            print(\"Epoch: %d, train loss: %f, val loss: %f, train ac: %f, val ac: %f\"\n",
    "                    %(epoch, loss.detach().numpy(), val_loss, train_accuracy, val_accuracy))\n",
    "        optimizer.zero_grad()  # Important\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, train loss: 0.356938, val loss: 1.006602, train ac: 0.992857, val ac: 0.794000\n",
      "Epoch: 10, train loss: 0.389274, val loss: 0.944934, train ac: 1.000000, val ac: 0.808000\n",
      "Epoch: 20, train loss: 0.341875, val loss: 0.957859, train ac: 0.992857, val ac: 0.782000\n",
      "Epoch: 30, train loss: 0.321970, val loss: 0.974770, train ac: 0.992857, val ac: 0.814000\n",
      "Epoch: 40, train loss: 0.308768, val loss: 0.994999, train ac: 1.000000, val ac: 0.794000\n",
      "Epoch: 50, train loss: 0.319464, val loss: 0.940084, train ac: 0.992857, val ac: 0.800000\n",
      "Epoch: 60, train loss: 0.339615, val loss: 0.918505, train ac: 1.000000, val ac: 0.798000\n",
      "Epoch: 70, train loss: 0.297739, val loss: 0.914063, train ac: 1.000000, val ac: 0.796000\n",
      "Epoch: 80, train loss: 0.302142, val loss: 0.914630, train ac: 1.000000, val ac: 0.804000\n",
      "Epoch: 90, train loss: 0.268684, val loss: 0.904370, train ac: 1.000000, val ac: 0.798000\n",
      "Epoch: 100, train loss: 0.277776, val loss: 0.978261, train ac: 1.000000, val ac: 0.796000\n",
      "Epoch: 110, train loss: 0.228860, val loss: 0.961943, train ac: 1.000000, val ac: 0.796000\n",
      "Epoch: 120, train loss: 0.257334, val loss: 0.953806, train ac: 1.000000, val ac: 0.796000\n",
      "Epoch: 130, train loss: 0.309855, val loss: 0.878085, train ac: 1.000000, val ac: 0.792000\n",
      "Epoch: 140, train loss: 0.274384, val loss: 0.911737, train ac: 0.992857, val ac: 0.800000\n",
      "Epoch: 150, train loss: 0.256738, val loss: 0.912487, train ac: 1.000000, val ac: 0.796000\n",
      "Epoch: 160, train loss: 0.239436, val loss: 0.908457, train ac: 1.000000, val ac: 0.802000\n",
      "Epoch: 170, train loss: 0.244403, val loss: 0.983555, train ac: 1.000000, val ac: 0.786000\n",
      "Epoch: 180, train loss: 0.277066, val loss: 0.943758, train ac: 1.000000, val ac: 0.804000\n",
      "Epoch: 190, train loss: 0.240577, val loss: 0.928277, train ac: 1.000000, val ac: 0.796000\n",
      "Epoch: 200, train loss: 0.216035, val loss: 0.929703, train ac: 1.000000, val ac: 0.794000\n"
     ]
    }
   ],
   "source": [
    "train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs224w",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
